% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{31}

\title{Self-Driving Assistant in Computer Simulation 
Environment}

\titlerunning{ENGN4528 Group \ECCV18SubNumber}

\authorrunning{ENGN4528 Group \ECCV18SubNumber}

\author{Zhiyuan Chen, Xingyuan Xu, Qiusi Xiang, Bisyri 
Hisham, Kavinenh Mohanraj}
\institute{Australian National University}


\maketitle

% FORMAT GUIDE
% No more than 60 characters in a single line [1][2]
% All lines must ends with a whitespace
% Every section ends with two empty lines
% Every subsection ends with a empty line
% [1]: Punctuation at the end of the line does not count
% [2]: Whitespace at the end of the line does not count

\begin{abstract}
This project implements a self-driving assistant in 
computer simulation environment with Spatial CNN for lane 
line detection and Mask R-CNN for objects detection on a 
distributed system based on RabbitMQ, Docker, and 
Kubernetes. 

\keywords Self-Driving, Spatial CNN, Mask R-CNN, 
Distributed System
\end{abstract}


\section{Introduction}
Self-driving technologies have been used in airplanes 
(known as AP for auto pilot), and trains (known as ATO for 
automatic train operation) for decades. However, as road 
traffic is far more complex, self-driving cars have never 
been in commercial use. Thanks to the development of 
machine learning, computer vision and most importantly the 
hardwares, self-driving cars do not seem to be impossible 
today. Thus, we have designed a self-driving assistant 
program which detects the lane line and objects in a 
computer simulation environment. 

Convolutional Neural Network (CNN) 


\section{Manager Process}
\subsection{Screenshot}
Taking a screenshot can be harder than it looks. 
ImageGrab.grab() from PIL is a good approach. However, 
it costs 300-400 ms to take a single screenshot which is 
obviously unacceptable. We have tested many different 
functions from different packages, as a result, Python-MSS 
is much faster than any other packages in comparison with 
an average speed of 60 fps. Thus, we used Python-MSS to 
capture screenshot at last. 

\subsection{Encode \& Decode}
As we used message queue as our message-oriented middleware, 
we must encode(serialize) the image prior to transmission 
and decode(deserialize) upon receiving messages. We tried 
three different ways to encode our image, JSON, pickle, 
and imencode. 

JSON is widely used in message queue, however, as ndarray 
used for image cannot be directly converted to JSON, we had 
to serialize the image first. Multiple attempts have been 
made, we succeeded in encoding the image in milliseconds at 
last, unfortunately, we could not decode the image fast 
enough and we had to abandon JSON. 

Directly pack the image using pickle can be another method, 
we tried cPickle here instead for better performance. The 
speed of encode and decode were the best among all other 
methods, however, the size of the message were also several 
times larger than the size of the image. Messages of this 
size would place heavy burdens on the network, therefore, 
cPickle was finally deprecated. 

The Joint Photographic Experts Group issued a synonym 
standard for still pictures compression in 1992. After 
27 years, the JPEG algorithm had became the most commonly 
used algorithm in image compression on this planet. OpenCV 
provides a built-in function, i.e., imencode, which 
converts an image from ndarray to byte stream within 
milliseconds. And more importantly, thanks to JPEG 
algorithm, the size of message are also ideal. 

Therefore, we were managed to reduce the transmission 
latency (i.e., encode image, transmit to worker, decode 
image, encode image, and transmit to manager), within 0.05 
seconds.

\begin{figure}
    \centering
    \includegraphics[height=8cm]{reference/latency}
    \label{fig:MQLatency}
    \caption{Message Queue Latency.}
\end{figure}


\section{Worker Process}

\subsection{Lane Line Detection}
Lane line detection has always been one of the most 
important parts of self-driving. This is easy to understand 
since driving off road usually means car crash and severe 
injuries. 

The traditional strategy of lane line detection used affine 
transformation to obtain the aerial view of the lane line. 
Then, edge detection, e.g. Canny, and Sobel, as well as 
gradient shall be performed to enhance the image. After the 
image process, sliding windows are used to locate the lane 
line. Lastly, project the lane line back to the original 
image, and calculate the radius and distance to center are 
to be performed. 

However, the traditional strategy does not have steady 
outputs, it would be easily influenced by curved roads, 
shadows, and others. Furthermore, it can neither process 
rural roads or trails without actual lines on the ground. 
Thus, most of the modern lane line detection algorithms 
use convolutional neural network (CNN) instead, and we 
are no exception. 

We compared two different CNN, i.e., LaneNet, and Spatial 
CNN in this project to achieve the best performance. 

LaneNet was suggested by Wang et al.\cite{LaneNet} in 2018. 
They proposed to cast the lane detection problem as an 
instance segmentation problem so that it can be trained 
end-to-end which could better cope with lane changes. 
They splited the image to background and lanes instead of 
assigning different classes to different lanes. By doing so, 
they were able to solve the problems of lane switching and 
limitation of the number of lanes. They started by binary 
segment the lane line with background first. They then 
connected all ground-truth lane points together to form a 
connected line per lane so that it would work well even 
though the lane line were occluded by objects like vehicles. 
Then, they instance segmented the lane line with each other 
using a one-shot method based on distance metric learning. 
Such a method used a clustering loss function to cluster 
points into lanes by means of the distances between pixel 
embeddings.

 
\begin{figure}
    \centering
    \includegraphics[width=10cm]{reference/lanenet}
    \label{fig:LaneNet}
    \caption{Network architecture of LaneNet.\cite{LaneNet}}
\end{figure}

Spatial CNN (SCNN) was proposed by Pan et al.
\cite{SpatialCNN} in 2017. They generalized traditional 
deep layer-by-layer convolutions to slice-by-slice 
convolutions within feature maps, which could enable 
message passings between pixels across rows and columns in 
the same layer. This allowed space information to propagate 
in the same layer, and made SCNN much more suitable to 
recognize structural objects. As a result, Spatial CNN won 
the TuSimple Lane Detection Challenge with 96.53\% of 
accuracy. 

\begin{figure}
    \centering
    \includegraphics[width=10cm]{reference/scnn}
    \label{fig:SCNN}
    \caption{Network architecture of Spatial CNN.\cite{SpatialCNN}}
\end{figure}


\begin{table}[!htbp]
	\centering
	\caption{The table 1 compares the speed and accuracy of LaneNet and Spatial CNN on TuSimple dataset}
	\begin{tabular}{|l|c|c|}
		\hline 
		&LaneNet&Spatial CNN\\
		\hline  
		Accuracy (\%)&96.4&96.53\\
		\hline  
		Speed (fps)&52&90\\
		\hline 
	\end{tabular}
\end{table}

After comparison, we found that SCNN works better than 
LaneNet in both speed and accuracy. Thus, we chose to use 
the SCNN in our project. The results of Lane Line detection 
are listed in the second row of the images in figure 8, 
which are mostly ideal. However, the generalization ability 
of SCNN is not as good as MRCNN which will be introduce 
later. 


\subsection{Obstacle Detection}
Prior to the introduction of region-based convolutional 
neural networks (R-CNN) suggested by Girshick et al. 
\cite{RCNN} in 2013, visual recognition tasks had been 
based considerably on the use of histograms of oriented 
gradients (HOG)\cite{HOG}, and Scale-Invariant Feature 
Transform (SIFT)\cite{SIFT}. However, as the recognition of 
brain occurs in several stages downstream, we could assume 
that a hierarchical, multi-stage algorithm can do better in 
the recognition task. 

The work of Girshick et al. on R-CNN showed us a new 
approach in pattern recognition. Compared to classification 
of image, recognition needs to localize the objects first 
which was considered as a regression problem. They used 
selective search to extract around 2000 region proposals. 
Then, they used a CNN to extract a 4096-dimensional feature 
vector from each region proposal. After that, they computed 
features by forward propagating a mean-subtracted 227 Ã— 227 
RGB image through five convolutional layers, and two fully 
connected layers. At last, they used a support vector 
machine (SVM) to classify the feature vectors, and a 
Bounding-box regression to get the ground truth boxes. 

\begin{figure}
    \centering
    \includegraphics[width=10cm]{reference/rcnn}
    \label{fig:RCNN}
    \caption{Network architecture of R-CNN\cite{RCNN}}
\end{figure}

We compared two different variants of R-CNN, i.e., Faster 
R-CNN, and Mask R-CNN in this project to achieve the best 
performance. 

Faster R-CNN\cite{FasterRCNN} (FRCNN) was suggested by Ren 
et al. in 2015. Similar to R-CNN, FRCNN used CNN to extract 
features from region proposals. However, FRCNN used a 
region proposal network (RPN) instead of selective search 
to localize the region proposals. In addition, a ROI 
pooling layer was added between RPN and CNN, which was used 
to collect proposals, and to calculate the proposal feature 
maps for extracting features. Besides, FRCNN used 
fully-connected layer, and softmax to classify the features 
that is similar Fast R-CNN instead of SVM. 

\begin{figure}
    \centering
    \includegraphics[width=10cm]{reference/frcnn}
    \label{fig:FRCNN}
    \caption{Network architecture of FRCNN\cite{FasterRCNN}}
\end{figure}

Mask R-CNN\cite{MaskRCNN} (MRCNN) was proposed by He et al. 
in 2018. As there would be mis-alignment between feature 
image and original image in Faster R-CNN due to ROI polling, 
MRCNN used ROIAlign instead of ROI pooling. In addition, 
MRCNN adds a convolutional layer after ROIAlign to predicate 
the mask. 

\begin{figure}
    \centering
    \includegraphics[width=10cm]{reference/mrcnn}
    \label{fig:MRCNN}
    \caption{Network architecture of MRCNN\cite{MaskRCNN}}
\end{figure}

Since the MRCNN outputs the result of segmentation and 
detection in parallel, the result can be better. Thus, 
we chose to use MRCNN in this project to detect objects.
The results of Object detection are listed in the third row 
of the images in figure 8, which are nearely perfect. 
Strong generalization ability makes it cope excellently 
with every dataset we tested. 


\section{Distributed System}

\subsection{Message-Oriented Middleware}
The core of the whole distributed system is 
Message-oriented middleware (MOM) since it is used to 
transmit data between each cluster. MOM delivers messages 
asynchronously which benefits us a lot. It allows us to 
remove dependencies between each algorithm and decouple our 
program. Moreover, it also increases the reliability of our 
program, since the system would continue to work even 
though one or more of the algorithms encounter fatal 
problems, and unable to recover. 

Message queue (MQ) was used in our program as the MOM. We 
compared many MQs, including kafka, RabbitMQ, NATS, and 
Redis to find the best performed MQ in latency. As 
Advanced message queueing protocol (AMQP) implementation, 
kafka and RabbitMQ are much slower than Redis and NATS in 
both 1 KiB and 5 KiB text. However, with regards to 1 MiB 
text, RabbitMQ and kafka are 200 times faster than NATS and 
Redis in 99.99th percentile. Thus, we chose RabbitMQ as our 
MOM since it works slightly better in large message 
transmission. In addition, to improve the real-time 
capabilities, a Time To Live limit is set to 1 millisecond.

\begin{figure}
	\centering
	\includegraphics[width=10cm]{reference/mq}
	\label{fig:MQ}
	\caption{Latency between different Message Queue\cite{MQ}}
\end{figure}


\subsection{Containerization}
Containerization has been widely used in many industries. 
Compared to traditional virtual machine, the performance, 
and resources loss of containerization are reduced to large 
extents which benefits from removing the guest OS, and 
hardware virtualization. Moreover, containerization would 
also reduce the effort to deploy in future works. 

Docker, the most popular container platform, is used here 
in this project to containerize our algorithms. In contrast 
with other containerization strategy, e.g. LXD, Docker 
focuses on running programs rather than running a whole 
system which makes itself easier to use. In addition, 
Docker also supports multiple system which is more 
convenient for development. And most importantly, the 
nvidia-docker makes Docker more compatible with neural 
networks.

\subsection{Container Orchestration}
As our application runs on multiple containers, a container 
orchestration platform which could help us manage and scale 
our containers is also necessary. Docker Swarm was 
implemented first as it is built in the Docker CE but soon 
deprecated since Docker Swarm does not provide basic 
health-checks and auto-rollback. Rancher provides more 
useful features, however, since Kubernetes has better 
support in nvidia-docker, we chose Kubernetes as our 
container orchestration platform at last.


\section{Conclusion}
In this project, we implemented a self-driving assistant 
program which detects the lane lines and objects. We 
compared different algorithms and used the best performed 
model as for now. In addition, with the advanced tools of 
RabbitMQ, Docker, and Kubernetes, we implemented a 
distributed system to manage and run our algorithms. We 
designed this system with proper exception handling and log 
system to avoid failures as much as possible, and to 
capture exception information in case of extreme situations 
for future improvements. As a result, our program is able 
to run with high performances and high availabilities. 

\subsection{Future Work}
In the future, we would like to improve our algorithms by 
transfer learning. Besides, we spent 37 milliseconds on 
average to transmit data between manager and worker, which 
suggests that there is still space for improvements. 

\subsection{Acknowledgement}
We would like to offer our sincerest appreciation to 
Hondong Li, the lecturer and convenor of this course as 
well as Ziang Cheng, and Kaiyue Lu, the tutor of this 
course for their help. We would also like to appreciate 
Yu Cui from the Peking University for her continued support, 
without whom, we could not finish this project. 


\section{Appendix}
\begin{figure}[!htb]
	\minipage{0.32\textwidth}
	Original Image
	\endminipage\hfill
	\minipage{0.32\textwidth}
	Lane Line Detection Result
	\endminipage\hfill
	\minipage{0.32\textwidth}
	Object Detection Result
	\endminipage

    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{result/w000059.jpg}
    \endminipage\hfill
    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{result/w000059-lane.jpg}
    \endminipage\hfill
    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{result/w000059-obj.jpg}
    \endminipage

	% \minipage{0.32\textwidth}
	% \includegraphics[width=\linewidth]{result/w000066.jpg}
	% \endminipage\hfill
	% \minipage{0.32\textwidth}
	% \includegraphics[width=\linewidth]{result/w000066-lane.jpg}
	% \endminipage\hfill
	% \minipage{0.32\textwidth}
	% \includegraphics[width=\linewidth]{result/w000066-obj.jpg}
	% \endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000092.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000092-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000092-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000106.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000106-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000106-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000107.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000107-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000107-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000128.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000128-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}%
	\includegraphics[width=\linewidth]{result/w000128-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000131.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000131-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000131-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000139.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000139-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/w000139-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/00810.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/00810-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/00810-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/01290.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/01290-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/01290-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/16.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/16-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/16-obj.jpg}
	\endminipage

	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/22.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/22-lane.jpg}
	\endminipage\hfill
	\minipage{0.32\textwidth}
	\includegraphics[width=\linewidth]{result/22-obj.jpg}
	\endminipage
\caption{Example result on ETS2, CULane, and TuSimple dataset}
\end{figure}
\clearpage


\bibliographystyle{splncs}
\bibliography{bib}
\end{document}
